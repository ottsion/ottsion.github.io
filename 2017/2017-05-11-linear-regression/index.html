<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta http-equiv="content-type" content="text/html;charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="robots" content="noodp"/>
  <meta name="author" content="Fengcai.Sun">
  
  
  
  <link rel="prev" href="https://ottsion.github.io/2017/2017-05-11-logistic-regression/" />
  <link rel="next" href="https://ottsion.github.io/2017/2017-05-11-shengcheng-model-panding-model/" />
  <link rel="canonical" href="https://ottsion.github.io/2017/2017-05-11-linear-regression/" />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <title>
       
       
           线性回归 | 静心明志
       
  </title>
  <meta name="title" content="线性回归 | 静心明志">
    
  
  <link rel="stylesheet" href="/font/iconfont.css">
  <link rel="stylesheet" href="/css/main.min.css">


  
  
 

<script type="application/ld+json">
 "@context" : "http://schema.org",
    "@type" : "BlogPosting",
    "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "https:\/\/ottsion.github.io"
    },
    "articleSection" : "posts",
    "name" : "线性回归",
    "headline" : "线性回归",
    "description" : "链接: 1. 线性回归总结 2. 正则化 3. 逻辑回归 4. Boosting 5. Adaboost算法\n一. 模型介绍 线性回归简而言之就是在平面中用一条直线去拟合一些点数据,在三维空间中就是用一个平面去拟合三维中的数据,而我们要做的就是寻找出一条最佳的线段或者平面去拟合数据,当然高维情况类似去寻找超平面。 初中的时候我们就学习过一元一次方程,那就是一个简单的拟合过程,只不过那个是完全可以拟合在一条线上,现在要做的是在有误差或者数据非线性排列的情况下,我们只能去尽力找出一条最佳的拟合线路:\nimport numpy as np import matplotlib.pyplot as plt x = np.arange(1,10,2).reshape(-1,1) y = a*2 plt.plot(x,y,\x27r-\x27,linewidth=2, label=u\x26quot;线性拟合\x26quot;) plt.plot(x,y, \x27bo\x27) plt.show()  import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression x = np.arange(1,10,2).reshape(-1,1) y = a*2 \x2b np.random.randn(5) linear_model = LinearRegression() linear_model.fit(x.reshape(-1, 1),y) y_pre = linear_model.predict(x) plt.plot(x,y_pre,\x27r-\x27,linewidth=2, label=u\x26quot;线性拟合\x26quot;) plt.plot(x,y, \x27bo\x27) plt.show()  定义一下一些符号表达，我们通常习惯用X=(x1,x2,\x26hellip;,xn)T∈ℝn×p表示数据矩阵，其中xi∈ℝp表示一个p维度长的数据样本；y=(y1,y2,\x26hellip;,yn)T∈ℝn表示数据的label，这里只考虑每个样本一类的情况。",
    "inLanguage" : "en-us",
    "author" : "caicai",
    "creator" : "caicai",
    "publisher": "caicai",
    "accountablePerson" : "caicai",
    "copyrightHolder" : "caicai",
    "copyrightYear" : "2017",
    "datePublished": "2017-05-11 23:59:59 \x2b0000 \x2b0000",
    "dateModified" : "2017-05-11 23:59:59 \x2b0000 \x2b0000",
    "url" : "https:\/\/ottsion.github.io\/2017\/2017-05-11-linear-regression\/",
    "wordCount" : "175",
    "keywords" : [ "ML,regression", "静心明志"]
}
</script>

</head>

  




  <body class="">
    <div class="wrapper">
        <nav class="navbar">
    <div class="container">
        <div class="navbar-header header-logo">
        	<a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-xihuan"></i></a>&nbsp;<a href="https://ottsion.github.io">静心明志</a>
        </div>
        <div class="menu navbar-right">
                
                
                <a class="menu-item" href="/posts/" title="">Blog</a>
                
                <a class="menu-item" href="/categories/" title="">Categories</a>
                
                <a class="menu-item" href="/tags/" title="">Tags</a>
                
                <a class="menu-item" href="/about/" title="">About</a>
                
                <a class="menu-item" href="/about/" title=""></a>
                
        </div>
    </div>
</nav>
<nav class="navbar-mobile" id="nav-mobile" style="display: none">
     <div class="container">
        <div class="navbar-header">
            <div>  <a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-xihuan"></i></a>&nbsp;<a href="https://ottsion.github.io">静心明志</a></div>
            <div class="menu-toggle">
                <span></span><span></span><span></span>
            </div>
        </div>
     
          <div class="menu" id="mobile-menu">
                
                
                <a class="menu-item" href="/posts/" title="">Blog</a>
                
                <a class="menu-item" href="/categories/" title="">Categories</a>
                
                <a class="menu-item" href="/tags/" title="">Tags</a>
                
                <a class="menu-item" href="/about/" title="">About</a>
                
                <a class="menu-item" href="/about/" title=""></a>
                
        </div>
    </div>
</nav>
    	 <main class="main">
          <div class="container">
      		
<article class="post-warp" itemscope itemtype="http://schema.org/Article">
    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">线性回归</h1>
        <div class="post-meta">
                Written by <a itemprop="name" href="https://ottsion.github.io" rel="author">caicai</a> with ♥ 
                <span class="post-time">
                on <time datetime=2017-05-11 itemprop="datePublished">May 11, 2017</time>
                </span>
                in
                <i class="iconfont icon-folder"></i>
                <span class="post-category">
                        <a href="https://ottsion.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"> 机器学习 </a>
                        
                </span>
        </div>
    </header>
    <div class="post-content">
        

        
            
        

        
        
     
          
          
          

          
          
          

          

<p>链接:
<a href="http://www.jianshu.com/p/25d650e5cb59" rel="nofollow noreferrer" target="_blank">1. 线性回归总结</a>
<a href="http://www.jianshu.com/p/dda5eb64f425" rel="nofollow noreferrer" target="_blank">2. 正则化</a>
<a href="http://www.jianshu.com/p/5ccc01385f40" rel="nofollow noreferrer" target="_blank">3. 逻辑回归</a>
<a href="http://www.jianshu.com/p/7128dde2af6f" rel="nofollow noreferrer" target="_blank">4. Boosting</a>
<a href="http://www.jianshu.com/p/b3f189767ad3" rel="nofollow noreferrer" target="_blank">5. Adaboost算法</a></p>

<hr />

<h2 id="一-模型介绍">一. 模型介绍</h2>

<p>线性回归简而言之就是在平面中用一条直线去拟合一些点数据,在三维空间中就是用一个平面去拟合三维中的数据,而我们要做的就是寻找出一条最佳的线段或者平面去拟合数据,当然高维情况类似去寻找超平面。
初中的时候我们就学习过一元一次方程,那就是一个简单的拟合过程,只不过那个是完全可以拟合在一条线上,现在要做的是在有误差或者数据非线性排列的情况下,我们只能去尽力找出一条最佳的拟合线路:</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
x = np.arange(1,10,2).reshape(-1,1)
y = a*2 
plt.plot(x,y,'r-',linewidth=2, label=u&quot;线性拟合&quot;)
plt.plot(x,y, 'bo')
plt.show()
</code></pre>

<p><figure><img src="/images/ring.svg" data-sizes="auto" data-src="http://upload-images.jianshu.io/upload_images/1070582-cda61f7178421a18.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="完全拟合,二元一次方程组求解;(数据线性情况下)" class="lazyload"><figcaption class="image-caption">完全拟合,二元一次方程组求解;(数据线性情况下)</figcaption></figure></p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

x = np.arange(1,10,2).reshape(-1,1)
y = a*2 + np.random.randn(5)
linear_model = LinearRegression()
linear_model.fit(x.reshape(-1, 1),y)
y_pre = linear_model.predict(x)
plt.plot(x,y_pre,'r-',linewidth=2, label=u&quot;线性拟合&quot;)
plt.plot(x,y, 'bo')
plt.show()
</code></pre>

<p><figure><img src="/images/ring.svg" data-sizes="auto" data-src="http://upload-images.jianshu.io/upload_images/1070582-29ac5417e759c672.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="线性回归;寻找最佳的拟合线段;(数据非线性情况下)" class="lazyload"><figcaption class="image-caption">线性回归;寻找最佳的拟合线段;(数据非线性情况下)</figcaption></figure></p>

<p>定义一下一些符号表达，我们通常习惯用X=(x1,x2,&hellip;,xn)T∈ℝn×p表示数据矩阵，其中xi∈ℝp表示一个p维度长的数据样本；y=(y1,y2,&hellip;,yn)T∈ℝn表示数据的label，这里只考虑每个样本一类的情况。</p>

<p>线性回归的模型是这样的，对于一个样本xi，它的输出值是其特征的线性组合：</p>

<p><figure><img src="/images/ring.svg" data-sizes="auto" data-src="http://upload-images.jianshu.io/upload_images/1070582-b9b74033fc121d09.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="线性表达式" class="lazyload"><figcaption class="image-caption">线性表达式</figcaption></figure></p>

<p>其中，w0称为截距，或者bias，通过设置X中X0=1,向量表达,简化了形式，因此实际上xi有p+1维度。
线性回归的目标是用预测结果尽可能地拟合目标label，</p>

<h2 id="二-损失函数">二. 损失函数</h2>

<p>从下图来直观理解一下线性回归优化的目标——图中线段距离（平方）的平均值，也就是最小化到分割面的距离和。
<figure><img src="/images/ring.svg" data-sizes="auto" data-src="http://upload-images.jianshu.io/upload_images/1070582-5a0a77beed3e1c18.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="三维中拟合后的情况" class="lazyload"><figcaption class="image-caption">三维中拟合后的情况</figcaption></figure></p>

<p>这里我们要做的是在随机放置权值W的情况下不断的优化程式,以达到效果最佳的情况,这里的优化指的是尽量让各个点拟合在所求超平面,如果不能拟合,也要使得其距离超平面最近为好,也就是可以定义损失函数</p>

<p><figure><img src="/images/ring.svg" data-sizes="auto" data-src="http://upload-images.jianshu.io/upload_images/1070582-3af679fd12c075bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="损失函数" class="lazyload"><figcaption class="image-caption">损失函数</figcaption></figure></p>

<h2 id="三-求解过程">三. 求解过程</h2>

<p>由于上述损失函数图形如下,我们这里选用两种方式去求解
<figure><img src="/images/ring.svg" data-sizes="auto" data-src="http://upload-images.jianshu.io/upload_images/1070582-d60ef19ae4a024bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="J(θ)平面" class="lazyload"><figcaption class="image-caption">J(θ)平面</figcaption></figure></p>

<h3 id="1-矩阵满秩">1. 矩阵满秩：</h3>

<p>这种形式下我们可以通过求解导数为零的方式求解算法，可以直接得到最后的Ｗ权值．
<figure><img src="/images/ring.svg" data-sizes="auto" data-src="http://upload-images.jianshu.io/upload_images/1070582-4137d5e50ef6eba9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="矩阵形式转换" class="lazyload"><figcaption class="image-caption">矩阵形式转换</figcaption></figure>
<figure><img src="/images/ring.svg" data-sizes="auto" data-src="http://upload-images.jianshu.io/upload_images/1070582-8227e6c4c7b1120b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="求解过程" class="lazyload"><figcaption class="image-caption">求解过程</figcaption></figure></p>

<h3 id="1-矩阵不满秩">1. 矩阵不满秩：</h3>

<p>　　当矩阵不满秩，无法求得上述的矩阵逆解，这里采用梯度下降法进行求解，梯度下降法通常分为三种形式：整体批次梯度下降法，随机梯度下降法和批量梯度下降法．
　　梯度下降算法是一种求局部最优解的方法，对于F(<strong>x</strong>)，在a点的梯度是F(<strong>x</strong>)增长最快的方向，那么它的相反方向则是该点下降最快的方向，具体参考<a href="http://en.wikipedia.org/wiki/Gradient_descent" rel="nofollow noreferrer" target="_blank">wikipedia</a>。
　　 原理：将函数比作一座山，我们站在某个山坡上，往四周看，从哪个方向向下走一小步，能够下降的最快；注意：当变量之间大小相差很大时，应该先将他们做处理，使得他们的值在同一个范围，这样比较准确。
    1）首先对θ赋值，这个值可以是随机的，也可以让θ是一个全零的向量。
    2）改变θ的值，使得J(θ)按梯度下降的方向进行减少。
　描述一下梯度减少的过程，对于我们的函数J(θ)求偏导J： 
<figure><img src="/images/ring.svg" data-sizes="auto" data-src="http://upload-images.jianshu.io/upload_images/1070582-185126423463544d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" class="lazyload"><figcaption class="image-caption"></figcaption></figure>
下图表达出解得最终结果与初始值相关：
<figure><img src="/images/ring.svg" data-sizes="auto" data-src="http://upload-images.jianshu.io/upload_images/1070582-3a38dff615044582.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="梯度下降方式" class="lazyload"><figcaption class="image-caption">梯度下降方式</figcaption></figure></p>

<p>在这里我们需要进行的是：
<figure><img src="/images/ring.svg" data-sizes="auto" data-src="http://upload-images.jianshu.io/upload_images/1070582-2d3453e9127d080f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="迭代公式" class="lazyload"><figcaption class="image-caption">迭代公式</figcaption></figure></p>

<p>　　在批量梯度下降中，每一次参数更新都会遍历全部的训练数据{x1, y1}, {x2, y2}, &hellip; , {xm,ym}对于线性回归问题，这种方法可以得到一个全局最优解，但是当样本数据量很大的时候，会非常耗时。</p>

<h2 id="多项式回归">多项式回归</h2>

<p>我们可以通过设计高阶特征在线性回归的基础上实现多项式回归。对于线性回归的hypothesis,
<figure><img src="/images/ring.svg" data-sizes="auto" data-src="http://upload-images.jianshu.io/upload_images/1070582-dc5bc59068a9dc44.latex?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" class="lazyload"><figcaption class="image-caption"></figcaption></figure>
令$x2=x1^2,x3=x1^3$,实现了一个3次多项式的回归：
<figure><img src="/images/ring.svg" data-sizes="auto" data-src="http://upload-images.jianshu.io/upload_images/1070582-c19a0861cca368f4.latex?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" class="lazyload"><figcaption class="image-caption"></figcaption></figure>
这也是用线性模型实现非线性的常用方法。这种方式能够拟合出非线性的超平面，有时候表现出很好的特性</p>

<pre><code>import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
X_train = [[6], [8], [10], [14], [18]]
y_train = [[7], [9], [13], [17.5], [18]]
X_test = [[6], [8], [11], [16]]
y_test = [[8], [12], [15], [18]]

quadratic_featurizer = PolynomialFeatures(degree=2)
X_train_quadratic = quadratic_featurizer.fit_transform(X_train)
X_test_quadratic = quadratic_featurizer.transform(X_test)
xx = np.linspace(0, 26, 100)
regressor_quadratic = LinearRegression()
regressor_quadratic.fit(X_train_quadratic, y_train)
xx_quadratic = quadratic_featurizer.transform(xx.reshape(xx.shape[0], 1))
plt.plot(xx, regressor_quadratic.predict(xx_quadratic), 'r-')
plt.show()
</code></pre>

<p><figure><img src="/images/ring.svg" data-sizes="auto" data-src="http://upload-images.jianshu.io/upload_images/1070582-eab9f5bbe7b5ae18.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="多项式回归与一般线性回归对比" class="lazyload"><figcaption class="image-caption">多项式回归与一般线性回归对比</figcaption></figure></p>

<p><a href="https://github.com/ottsion/machine_learning/tree/master/LinearRegression" rel="nofollow noreferrer" target="_blank">线性回归代码地址</a></p>

<p>参考:
<a href="http://blog.csdn.net/xbinworld/article/details/43919445" rel="nofollow noreferrer" target="_blank">机器学习方法：回归（一）：线性回归Linear regression</a>
<a href="http://www.cnblogs.com/GuoJiaSheng/p/3928160.html" rel="nofollow noreferrer" target="_blank">机器学习&mdash;&ndash;线性回归浅谈（Linear Regression）</a>
<a href="http://www.cnblogs.com/llhthinker/p/5248586.html" rel="nofollow noreferrer" target="_blank">Stanford机器学习笔记-1.线性回归</a>
<a href="http://www.jianshu.com/p/0396cb1a471c" rel="nofollow noreferrer" target="_blank">线性回归</a></p>

    </div>

    <div class="post-copyright">
             
            <p class="copyright-item">
                <span>Author:</span>
                <span>Fengcai.Sun </span>
                </p>
            
           
             
            <p class="copyright-item">
                    <span>Link:</span>
                    <a href=https://ottsion.github.io/2017/2017-05-11-linear-regression/>https://ottsion.github.io/2017/2017-05-11-linear-regression/</span>
            </p>
            
             
            <p class="copyright-item lincese">
                本文采用<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可
            </p>
            
    </div>

  
    <div class="post-tags">
        
            <section>
            <i class="iconfont icon-tag"></i>Tag(s): 
            
            <span class="tag"><a href="https://ottsion.github.io/tags/mlregression/">
                    #ML,regression</a></span>
            
            </section>
        
        <section>
                <a href="javascript:window.history.back();">back</a></span> · 
                <span><a href="https://ottsion.github.io">home</a></span>
        </section>
    </div>

    <div class="post-nav">
        
        <a href="https://ottsion.github.io/2017/2017-05-11-logistic-regression/" class="prev" rel="prev" title="逻辑回归"><i class="iconfont icon-left"></i>&nbsp;逻辑回归</a>
         
        
        <a href="https://ottsion.github.io/2017/2017-05-11-shengcheng-model-panding-model/" class="next" rel="next" title="生成模型与判别模型">生成模型与判别模型&nbsp;<i class="iconfont icon-right"></i></a>
        
    </div>

    <div class="post-comment">
          
                 
          
    </div>
</article>
          </div>
		   </main>
      <footer class="footer">
    <div class="copyright">
        &copy;
        
        <span itemprop="copyrightYear">2019 - 2020</span>
        
        <span class="with-love">
    	 <i class="iconfont icon-love"></i> 
         </span>
         
            <span class="author" itemprop="copyrightHolder"><a href="https://ottsion.github.io">Fengcai.Sun</a> | </span> 
         

         
		  <span>Powered by <a href="https://gohugo.io/" target="_blank" rel="external nofollow">Hugo</a> & <a href="https://github.com/liuzc/leaveit" target="_blank" rel="external nofollow">LeaveIt</a></span> 
    </div>
</footer>












    
     <link href="//lib.baomitu.com/lightgallery/1.6.11/css/lightgallery.min.css" rel="stylesheet">  
      
     <script src="/js/vendor_gallery.min.js" async="" ></script>
    
  



     </div>
  </body>
</html>
