<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Gibbs采样 on 静心明志</title>
    <link>https://ottsion.github.io/tags/gibbs%E9%87%87%E6%A0%B7/</link>
    <description>Recent content in Gibbs采样 on 静心明志</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 24 Feb 2020 15:57:32 +0800</lastBuildDate>
    
	<atom:link href="https://ottsion.github.io/tags/gibbs%E9%87%87%E6%A0%B7/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>LDA主题模型</title>
      <link>https://ottsion.github.io/2020/lda/</link>
      <pubDate>Mon, 24 Feb 2020 15:57:32 +0800</pubDate>
      
      <guid>https://ottsion.github.io/2020/lda/</guid>
      <description>LDA详解  学习LDA，首先应该要了解相关的概率分布，了解为什么使用Dirichlet分布和吉布斯采样，然后明白LDA源自于PLSA， 再学习LDA的建模流程，所以下文主要从这几个方向说起。
 第一话：概率分布 核心点：Dirichlet分布其实也是采样出一个值（向量），从这个意义上来说，它其实和其它分布并无太大不同，那为什么大家都说Dirichlet分布式分布的分布呢？**因为Dirichlet分布出现的场景，总是用于生成别的分布（更确切地说，总是用于生成Multinomial分布）**Dirichlet分布得到的向量各个分量的和是1，这个向量可以作为Multinomial分布的参数，所以我们说Dirichlet能够生成Multinomial分布，也就是分布的分布
1.1. 多项分布 多项式分布(Multinomial Distribution)是二项式分布的推广。二项式做n次伯努利实验，规定了每次试验的结果只有两个，如果现在还是做n次试验，只不过每次试验的结果可以有多m个，且m个结果发生的概率互斥且和为1，则发生其中一个结果X次的概率就是多项式分布。
在二项分布中，事件只有两种可能，假定第一种可能发生概率为$p$，则另一件为$1-p$，那么n此实验后后k次发生第一种事件的概率为$P{X=k}=C_{n}^{k}p^{k}(i-p)^{n-k}$
在多项式分布中，事件是有多个的，且每个事件会产生$k_i$次的概率是：$P{X_1=k_1,X_2=k_2,,,,X_n=k_n}=\frac{n!}{k_1!k_2!&amp;hellip;k_n!}\prod_{i=0}^{n}p_{i}^{k_i}$ 其中$\sum_{i=0}^{n}k_i=n$
多项分布对其每一个结果都有均值和方差，分别为 :
$E[x_i]=np_i$
$var[x_i]=np_i(1-p_i)$
1.2. Gamma分布 $\Gamma(x)=\int_{0}^{\infty}t^{x-1}e^{-t}dt$
它的最大特点:$\Gamma(x+1)=x\Gamma(x)$
据PRML第71页(2.14)式，Gamma函数在Beta分布和Dirichlet分布中起到了归一化的作用。
1.3. Dirichlet分布 Dirichlet分布是关于定义在区间[0,1]上的多个随机变量的联合概率分布 ,假定有d个变量$\mu_i$,并且$\sum_{i=1}^{d}\mu_i=1$ ，记$\mu=(\mu_1,\mu_2,&amp;hellip;\mu_d)$，每个$\mu_i$对应一个参数$\alpha_i&amp;gt;0$ ,记$\alpha=(\alpha_1,\alpha_2,&amp;hellip;\alpha_d)$，$\widehat\alpha=\sum_{i=1}^{d}\alpha_i$，那么它的概率密度函数为：$p(\mu|\alpha)=Dir(\mu|\alpha)=\frac{ \Gamma(\widehat\alpha)}{\Gamma(\alpha_1)\Gamma(\alpha_2)&amp;hellip;\Gamma(\alpha_d)}\prod_{i=1}^{d}\mu_{i}^{\alpha_{i-1}}$
Dirichlet分布的每一个随机变量具有统计量如下：
$E[\mu_i]=\frac{\alpha_i}{\widehat\alpha}$
$var(\mu_i)=\frac{\alpha_i(\widehat\alpha-\alpha_i)}{\widehat\alpha^{2}(\widehat\alpha+1)}$
$cov(\mu_i, \mu_j)=\frac{\alpha_i\alpha_j}{\widehat\alpha^{2}(\widehat\alpha+1)}$
由于Dirichlet分布描述的是多个定义于区间[0,1]的随机变量的概率分布，所以通常将其用作多项分布参数$\mu_{i}$ 的概率分布，也就是上面1中的每个事件的概率值。
1.4. Beta分布 Beta分布描述的是定义在区间[0,1]上随机变量的概率分布， beta分布的定义域是(0,1) 这就跟概率的范围是一样的。 由两个参数$\alpha$ &amp;gt;0和$\beta$ &amp;gt;0决定，通常记为$\mu∼Beta(\mu|\alpha,\beta)$，其概率密度函数如下 :$P(\mu|\alpha,\beta)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)+\Gamma(\beta)}\mu^{\alpha-1}(1-\mu)^{\beta-1}$
$E[\mu]=\frac{\alpha}{\alpha+\beta}$
$var[\mu]=\frac{\alpha\beta}{(\alpha+\beta)^{2}(\alpha+\beta+1)}$
这里举个例子，原文在最下面参考中，说是判断一个人打球的击中率，正常人一般在0.277左右，当有一场比赛该人员只打了一次就中了，如果这样算就是100%是不对的，他应该是在以前水平的基础上建模，然后用这次的数据进行修正；那么以前的击中率也不应该直接是一个值，在未知的常态下它应该是一个概率分布，比如最大可能是0.27这样子。如下的Beta分布表示当$\alpha=81$,$\beta=219$时的分布，它表示的是概率分布的概率，这里举例来说x=0.27的概率（可能性）最高，正好用来表示过去该人员的一个水平概率分布。
其中$E[X]=\frac{\alpha}{\alpha+\beta}=0.27$ , 从图中可以看到这个分布主要落在了(0.2,0.35)间，这是从经验中得出的合理的范围。
当该人员新参加一场比赛, 假设一共打了300次，其中击中了100次，200次没击中，那么这一新分布就是： $beta(81+100, 219+200)$ , 注意到这个曲线变得更加尖，并且平移到了一个右边的位置，表示比平均水平要高。 而且通过这样的方式进行击中率计算，会小于直接按照频次计算，进行了平滑操作。
之所以上式直接相加是因为 beta分布与二项分布的共轭先验性质。这里也需要清楚多项分布和Dirichlet分布也属于共轭性质
​ 1.5. 吉布斯采样 在已知条件概率分布而不知联合概率分布的情况下，如何较为可靠的采样出一批数据。
MCMC：Markov链通过转移概率矩阵可以收敛到稳定的概率分布(类似于pagerank收敛)。这意味着MCMC可以借助Markov链的平稳分布特性模拟高维概率分布p(x)；当Markov链经过burn-in阶段，消除初始参数的影响，到达平稳状态后，每一次状态转移都可以生成待模拟分布的一个样本。
而Gibbs抽样是MCMC的一个特例，它交替的固定某一维度$x_i$，然后通过其他维度$x_j$的值来抽样该维度的值，注意，gibbs采样只对高维（2维以上）情况有效。
基本算法如下：
针对样本$X={x_1, x_2, &amp;hellip;, x_n}$共有n个维度，每次针对单个维度$x_k$ 固定住其他维度单独更新该维度，迭代T次后趋于平稳，即可进行采样。</description>
    </item>
    
  </channel>
</rss>