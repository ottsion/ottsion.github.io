<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ML, logistic on 静心明志</title>
    <link>https://ottsion.github.io/tags/ml-logistic/</link>
    <description>Recent content in ML, logistic on 静心明志</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 May 2017 23:59:59 +0000</lastBuildDate>
    
	<atom:link href="https://ottsion.github.io/tags/ml-logistic/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>逻辑回归</title>
      <link>https://ottsion.github.io/2017/2017-05-11-logistic-regression/</link>
      <pubDate>Thu, 11 May 2017 23:59:59 +0000</pubDate>
      
      <guid>https://ottsion.github.io/2017/2017-05-11-logistic-regression/</guid>
      <description>链接: 1. 线性回归总结 2. 正则化 3. 逻辑回归 4. Boosting 5. Adaboost算法
 线性回归是通过拟合来达到目的，而逻辑回归呢侧重的是探寻概率，它不去寻找拟合的超平面，而是去寻找某个数据的类别归属问题。
一. 预测函数 怎么去预测？我们知道Sigmoid函数是如上这样的，它的表现是：将数据归纳在0-1之间，那么我们能不能通过去计算出的结果去拟合这样一个概率，使之成为一种分类的依据？答案是肯定的。凡事的概率都在0和1之间，拟合了概率，就是拟合了判定条件。 二. 具体做法 我们知道线性回归是这样子的： 将Sigmoid函数加载到这个结果上，不就是将结果0-1概率化了么： 所以是这样子的：
三. 损失函数 这样一来针对已有数据那就是0/1问题，要么概率为1的数据，要么概率为0的数据：
对于我们来说概率可以简写合并成： 取其似然函数： 我们要做的是在最大似然估计就是求使 L(θ )取最大值时的θ 这里可以自己做主，我选用下面作为损失函数，要是最大似然估计最大，就要使J(θ)函数最小，通过不断的优化θ使得J函数最小，进而L函数概率最大，完成任务。 四. 求解过程 1. 梯度下降法：### 2. 矩阵法：### 通过依次求解A，E, θ得到最终解。（A为线性回归的θ*X）
参考： 逻辑回归 逻辑回归模型(Logistic Regression, LR)基础 - 文赛平 机器学习—逻辑回归理论简介</description>
    </item>
    
  </channel>
</rss>